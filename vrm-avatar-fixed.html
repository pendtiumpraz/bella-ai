<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>VRM Avatar - Fixed Version</title>
    <style>
        body {
            margin: 0;
            overflow: hidden;
            background: transparent;
        }
        #info {
            position: absolute;
            top: 10px;
            left: 10px;
            color: white;
            background: rgba(0,0,0,0.7);
            padding: 10px;
            border-radius: 5px;
            font-family: Arial, sans-serif;
        }
        #error {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            color: red;
            background: rgba(0,0,0,0.8);
            padding: 20px;
            border-radius: 10px;
            display: none;
        }
        canvas {
            display: block;
        }
    </style>
</head>
<body>
    <div id="info">Loading...</div>
    <div id="error"></div>

    <!-- Using CDN with specific versions that work together -->
    <script async src="https://unpkg.com/es-module-shims@1.6.3/dist/es-module-shims.js"></script>

    <script type="importmap">
    {
        "imports": {
            "three": "https://unpkg.com/three@0.158.0/build/three.module.js",
            "three/addons/": "https://unpkg.com/three@0.158.0/examples/jsm/"
        }
    }
    </script>

    <script type="module">
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { VRMLoaderPlugin, VRMUtils } from 'https://unpkg.com/@pixiv/three-vrm@2.0.7/lib/three-vrm.module.js';

        const info = document.getElementById('info');
        const errorDiv = document.getElementById('error');

        // Check if running in iframe
        console.log('VRM Avatar Fixed - Initializing...');

        // Scene setup
        const scene = new THREE.Scene();
        
        // Camera - Zoom in closer, focus on face
        const camera = new THREE.PerspectiveCamera(30, window.innerWidth / window.innerHeight, 0.1, 20);
        camera.position.set(0, 1.5, 1.5); // Y position at face level, Z closer for zoom in
        
        // Renderer
        const renderer = new THREE.WebGLRenderer({ 
            antialias: true,
            alpha: true 
        });
        renderer.setSize(window.innerWidth, window.innerHeight);
        renderer.setPixelRatio(window.devicePixelRatio);
        renderer.outputColorSpace = THREE.SRGBColorSpace;
        document.body.appendChild(renderer.domElement);
        
        // Lights
        const light = new THREE.DirectionalLight(0xffffff, 1);
        light.position.set(1, 1, 1);
        scene.add(light);
        
        const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
        scene.add(ambientLight);

        // VRM holder
        let currentVrm = null;
        const clock = new THREE.Clock();

        // Mouse controls and eye tracking
        let mouseX = 0;
        let mouseY = 0;
        let targetRotationY = 0;
        let currentRotationY = 0;
        let targetLookAtX = 0;
        let targetLookAtY = 0;
        let currentLookAtX = 0;
        let currentLookAtY = 0;
        
        // Create look at target
        const lookAtTarget = new THREE.Object3D();
        scene.add(lookAtTarget);

        // Track mouse for eye movement
        document.addEventListener('mousemove', (event) => {
            mouseX = (event.clientX / window.innerWidth) * 2 - 1;
            mouseY = -(event.clientY / window.innerHeight) * 2 + 1;
            
            // For body rotation when dragging
            if (event.buttons === 1) {
                targetRotationY = mouseX * Math.PI;
            }
            
            // Update look at target position
            targetLookAtX = mouseX * 2;
            targetLookAtY = mouseY * 0.5 + 1.5; // Match camera Y position
        });

        renderer.domElement.addEventListener('wheel', (event) => {
            camera.position.z += event.deltaY * 0.001;
            camera.position.z = Math.max(1, Math.min(5, camera.position.z));
        });

        // Load VRM
        async function loadVRM(url) {
            try {
                info.textContent = 'Loading VRM model...';
                
                const loader = new GLTFLoader();
                
                // Register VRM plugin
                loader.register((parser) => {
                    return new VRMLoaderPlugin(parser);
                });

                const gltf = await loader.loadAsync(url);
                
                const vrm = gltf.userData.vrm;
                
                if (!vrm) {
                    throw new Error('No VRM data found in file');
                }

                // Remove old model
                if (currentVrm) {
                    scene.remove(currentVrm.scene);
                    VRMUtils.deepDispose(currentVrm.scene);
                }

                // Add new model
                currentVrm = vrm;
                scene.add(vrm.scene);
                
                // Rotate model to face camera (180 degrees)
                vrm.scene.rotation.y = Math.PI;
                
                // Setup VRM lookAt
                if (vrm.lookAt) {
                    vrm.lookAt.target = lookAtTarget;
                    vrm.lookAt.autoUpdate = true;
                }
                
                // Reset to default pose (T-pose to relaxed pose)
                if (vrm.humanoid) {
                    // Lower arms straight down to sides
                    const leftUpperArm = vrm.humanoid.getNormalizedBoneNode('leftUpperArm');
                    const rightUpperArm = vrm.humanoid.getNormalizedBoneNode('rightUpperArm');
                    
                    if (leftUpperArm) {
                        leftUpperArm.rotation.z = Math.PI / 2.3; // About 78 degrees - arms straight down
                    }
                    if (rightUpperArm) {
                        rightUpperArm.rotation.z = -Math.PI / 2.3; // About -78 degrees - arms straight down
                    }
                    
                    // Keep arms straight, no elbow bend
                    const leftLowerArm = vrm.humanoid.getNormalizedBoneNode('leftLowerArm');
                    const rightLowerArm = vrm.humanoid.getNormalizedBoneNode('rightLowerArm');
                    
                    if (leftLowerArm) {
                        leftLowerArm.rotation.y = 0; // Straight
                    }
                    if (rightLowerArm) {
                        rightLowerArm.rotation.y = 0; // Straight
                    }
                }
                
                // Setup blendshapes for expressions
                if (vrm.expressionManager) {
                    console.log('Available expressions:', vrm.expressionManager.expressions.map(e => e.expressionName));
                }
                
                info.textContent = 'VRM loaded: ' + url.split('/').pop();
                console.log('VRM loaded successfully:', vrm);
                
            } catch (error) {
                console.error('Error loading VRM:', error);
                errorDiv.style.display = 'block';
                errorDiv.innerHTML = `
                    Failed to load VRM!<br>
                    ${error.message}<br>
                    <small>Check console for details</small>
                `;
                info.textContent = 'Error!';
            }
        }

        // Blink timer
        let blinkTimer = 0;
        let isBlinking = false;

        // Animation loop
        function animate() {
            requestAnimationFrame(animate);
            
            const deltaTime = clock.getDelta();
            
            // Smooth rotation
            currentRotationY += (targetRotationY - currentRotationY) * 0.05;
            
            if (currentVrm) {
                // Update VRM
                currentVrm.update(deltaTime);
                
                // Apply body rotation (keep facing forward + mouse rotation)
                currentVrm.scene.rotation.y = Math.PI + currentRotationY;
                
                // Smooth look at interpolation for eye tracking
                currentLookAtX += (targetLookAtX - currentLookAtX) * 0.1;
                currentLookAtY += (targetLookAtY - currentLookAtY) * 0.1;
                
                // Update look at target position
                if (!isSpeaking) {
                    // Follow mouse when not speaking
                    lookAtTarget.position.set(currentLookAtX, currentLookAtY, 2);
                } else {
                    // Look forward with natural head movement when speaking
                    const headSway = Math.sin(clock.elapsedTime * 2) * 0.1;
                    const headNod = Math.sin(clock.elapsedTime * 3) * 0.05;
                    lookAtTarget.position.set(headSway, 1.5 + headNod, 2);
                }
                
                // Breathing animation
                const breathing = Math.sin(clock.elapsedTime * 2) * 0.003;
                if (currentVrm.humanoid) {
                    const spine = currentVrm.humanoid.getNormalizedBoneNode('spine');
                    const chest = currentVrm.humanoid.getNormalizedBoneNode('upperChest') || 
                                currentVrm.humanoid.getNormalizedBoneNode('chest');
                    const hips = currentVrm.humanoid.getNormalizedBoneNode('hips');
                    
                    if (spine) {
                        spine.position.y = breathing;
                        
                        // Micro movements while speaking
                        if (isSpeaking) {
                            const torsoRotation = Math.sin(clock.elapsedTime * 1.5) * 0.05;
                            spine.rotation.y = torsoRotation;
                        }
                    }
                    if (chest) {
                        chest.position.y = breathing * 0.5;
                    }
                    
                    // Subtle idle motion - body swaying
                    const swayAmount = isSpeaking ? 0.03 : 0.015;
                    const sway = Math.sin(clock.elapsedTime * 0.7) * swayAmount;
                    currentVrm.scene.position.x = sway;
                    
                    // Hip movement when speaking
                    if (hips && isSpeaking) {
                        const hipSway = Math.sin(clock.elapsedTime * 0.8) * 0.01;
                        hips.position.x = hipSway;
                    }
                }
                
                // Auto blink
                blinkTimer += deltaTime;
                if (blinkTimer > 3 + Math.random() * 2 && !isBlinking) {
                    isBlinking = true;
                    if (currentVrm.expressionManager) {
                        const blinkExp = currentVrm.expressionManager.getValue('blink');
                        if (blinkExp !== undefined) {
                            currentVrm.expressionManager.setValue('blink', 1);
                            setTimeout(() => {
                                currentVrm.expressionManager.setValue('blink', 0);
                                isBlinking = false;
                            }, 150);
                        }
                    }
                    blinkTimer = 0;
                }
            }
            
            renderer.render(scene, camera);
        }
        animate();

        // Handle resize
        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });

        // Speaking animation
        let isSpeaking = false;
        let speakingTimer = 0;
        
        function startSpeaking(text, emotion = 'neutral') {
            isSpeaking = true;
            info.textContent = 'Speaking...';
            
            // Analyze text for vowels
            const vowels = text.match(/[aeiouAEIOU]/g) || [];
            const duration = Math.min(text.length * 60, 8000);
            
            let vowelIndex = 0;
            
            // Set facial expression if provided
            if (emotion !== 'neutral') {
                setExpression(emotion);
            }
            
            // Enhanced lip sync
            const lipSyncInterval = setInterval(() => {
                if (!currentVrm || !currentVrm.expressionManager) return;
                
                // Vowel to mouth shape mapping
                const vowelShapeMap = {
                    'a': 'aa',
                    'e': 'ee', 
                    'i': 'ih',
                    'o': 'oh',
                    'u': 'ou'
                };
                
                // Reset mouth shapes
                ['aa', 'ih', 'ou', 'ee', 'oh'].forEach(shape => {
                    if (currentVrm.expressionManager.getValue(shape) !== undefined) {
                        currentVrm.expressionManager.setValue(shape, 0);
                    }
                });
                
                // Get current vowel and map to shape
                if (vowelIndex < vowels.length) {
                    const currentVowel = vowels[vowelIndex].toLowerCase();
                    const shape = vowelShapeMap[currentVowel] || 'aa';
                    const intensity = 0.6 + Math.random() * 0.3;
                    
                    if (currentVrm.expressionManager.getValue(shape) !== undefined) {
                        currentVrm.expressionManager.setValue(shape, intensity);
                    }
                }
                
                vowelIndex++;
                if (vowelIndex >= vowels.length) {
                    vowelIndex = 0; // Loop if needed
                }
            }, 150);
            
            setTimeout(() => {
                clearInterval(lipSyncInterval);
                isSpeaking = false;
                
                // Close mouth smoothly
                if (currentVrm && currentVrm.expressionManager) {
                    ['aa', 'ih', 'ou', 'ee', 'oh'].forEach(shape => {
                        if (currentVrm.expressionManager.getValue(shape) !== undefined) {
                            currentVrm.expressionManager.setValue(shape, 0);
                        }
                    });
                }
                
                // Reset expression after speaking
                if (emotion !== 'neutral') {
                    setTimeout(() => {
                        setExpression('neutral');
                    }, 1000);
                }
                
                info.textContent = 'Ready';
            }, duration);
        }
        
        // Set expression
        function setExpression(emotion) {
            if (!currentVrm || !currentVrm.expressionManager) return;
            
            // Reset all expressions first
            ['happy', 'angry', 'sad', 'relaxed', 'surprised'].forEach(exp => {
                if (currentVrm.expressionManager.getValue(exp) !== undefined) {
                    currentVrm.expressionManager.setValue(exp, 0);
                }
            });
            
            // Set new expression
            if (currentVrm.expressionManager.getValue(emotion) !== undefined) {
                currentVrm.expressionManager.setValue(emotion, 1);
                
                // Auto reset after 3 seconds
                setTimeout(() => {
                    currentVrm.expressionManager.setValue(emotion, 0);
                }, 3000);
            }
        }

        // Listen for messages from parent
        window.addEventListener('message', (event) => {
            if (event.data.type === 'loadModel') {
                console.log('Received loadModel message:', event.data.path);
                loadVRM(event.data.path);
            } else if (event.data.type === 'speak') {
                console.log('Received speak message:', event.data.text);
                const emotion = event.data.emotion || 'neutral';
                startSpeaking(event.data.text, emotion);
            } else if (event.data.type === 'expression') {
                console.log('Received expression message:', event.data.emotion);
                setExpression(event.data.emotion);
            } else if (event.data.type === 'cleanup') {
                console.log('Received cleanup message');
                // Cleanup WebGL resources
                if (currentVrm) {
                    scene.remove(currentVrm.scene);
                    VRMUtils.deepDispose(currentVrm.scene);
                    currentVrm = null;
                }
                if (renderer) {
                    renderer.dispose();
                    renderer.forceContextLoss();
                }
            }
        });

        // Load default model - Galih Hoodie Esteh
        loadVRM('vtuber/Galih Hoodie Esteh.vrm');

        // Expose for debugging
        window.debugVRM = {
            scene,
            camera,
            renderer,
            currentVrm,
            loadVRM
        };
    </script>
</body>
</html>